{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overfitting and Regularization\n",
    "\n",
    "Understanding model fit is important for understanding the root cause for poor model accuracy. We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data.\n",
    "\n",
    "A model is *underfitting* the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples and the target values.\n",
    "\n",
    "A model is *overfitting* the training data when the model performs well on training data but does not perform well on the test data. This is because the model is simply memorizing the data it has seen and is unable to generalize to unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Data/ModelFit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model regularization** refers to the practice of tuning the preferred level of model complexity so that the model is better at generalizing. For Machine Learning models, regularization is typically achieved by constraining the parameters of a model.\n",
    "\n",
    "- **L1 regularization** adds absolute value of parameters as penalty term to the cost function. This forces the learning algorithm to not only fit the data but also keep the model parameters as small as possible. Linear regression with L1 regularization is called Lasso regression (least absolute shrinkage and selection operator). Its cost function is\n",
    "\n",
    "$J(\\theta) = \\sum_{i=1}^m (\\textbf{x}^{(i)}\\cdot \\theta^T - y^{(i)})^2 +\n",
    "\\alpha\\sum_{j=0}^{n}|\\theta_j|$.\n",
    "\n",
    "Here $\\alpha$ is a hyperparameter that controls how much you want to regularize the model. If $\\alpha=0$, then we will get back to plain linear regression model, whereas large value of $\\alpha$ will make parameter close to zero (and may cause model underfitting). It is important to choose $\\alpha$ properly.\n",
    "\n",
    "- **L2 regularization** adds squares of parameters as penalty term to the cost function. Linear regression with L2 regularization is called Ridge regression. Its cost function is\n",
    "\n",
    "$J(\\theta) = \\sum_{i=1}^m (\\textbf{x}^{(i)}\\cdot \\theta^T - y^{(i)})^2 +\n",
    "\\alpha\\sum_{j=0}^{n}\\theta_j^2$.\n",
    "\n",
    "- **Elastic Net** is a middle ground between L1 regularization and L2 regularization. Its regularization term is a mix of L1 term and L2 term. Linear regression with elastic net regulariztion has cost function\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^m (\\textbf{x}^{(i)}\\cdot \\theta^T - y^{(i)})^2 +\n",
    "r\\alpha\\sum_{j=0}^{n}|\\theta_j| \n",
    "+ \\frac{1-r}{2}\\alpha\\sum_{j=0}^{n}\\theta_j^2$.$\n",
    "\n",
    "#### L1 regularization vs. L2 regularization\n",
    "- Linear regression with L2 regularization has analytical solution\n",
    "\n",
    "$\\theta = (\\textbf{X}^T\\cdot \\textbf{X} + \\alpha \\textbf{A})^{-1}\\cdot\\textbf{X}\\cdot\\textbf{y}$,\n",
    "\n",
    "where matrix $\\textbf{A}$ is an $n\\times n$ identity matrix, except with a 0 on the top-left cell, corresponding to the constant term. \n",
    "\n",
    "- L1 regularization tends to make many parameters very close to zero, and thus will produce a more sparse model.\n",
    "\n",
    "- Computationally, L1 regularization is more efficient on sparse datasets, while L2 regularization is usually more efficient on non-sparse datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npEIY+JJUCANfkgph4EtSIf4fPwJQ+LsgXOQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1784d68dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An example on regularized models\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# generate dataset\n",
    "np.random.seed(42)\n",
    "m = 20  # number of instances\n",
    "X = 5 * np.random.rand(m, 1)  # inputs are uniformly distributed\n",
    "                              # on interval [0, 3).\n",
    "# target value equals 1 + 0.5x + x^2 + Gaussian error N(0, 100)\n",
    "y = 1 + 0.5 * X + X ** 2 + np.random.randn(m, 1) * 10\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build a polynomial regression model with no regularization \n",
    "# Use sklearn.preprocessing.PolynomialFeatures to transform\n",
    "# input X into X_poly that contains powers up to degree 20.\n",
    "\n",
    "\n",
    "# scale the input features, so that all powers have zero mean \n",
    "# and unit variance\n",
    "\n",
    "\n",
    "# Polynomial regression is just linear regression with\n",
    "# polynomial terms of features. \n",
    "# Use sklearn.linear_model.LinearRegression to fit X_poly and y\n",
    "\n",
    "\n",
    "# plot regression line and the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# polynomial regression model with L2 (Ridge) regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# choose different values of alpha's: 0, 0.05, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# polynomial regression with L1 (Lasso) regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# we can combine polynomial transform and linear regression \n",
    "# using sklearn.pipeline.Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification on MNIST\n",
    "How to represent multiple classes?\n",
    "- use a number to represent each class (0 for 0, 1 for 1, etc.)\n",
    "- one-hot encoding (0 -> (1, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n",
    "- binary encoding (4 -> (100)_2 -> (1, 0, 0))\n",
    "\n",
    "The following example uses numeric representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fetch MNIST data from mldata.org\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "# load data and target as X and y\n",
    "X, y = mnist['data'], mnist['target']\n",
    "# use sklearn.model_selection.train_test_split to split dataset\n",
    "# into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1/7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 1.33 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   12.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 0.73 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   10.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 0.33 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 0.59 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 0.45 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   10.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 0.86 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 0.60 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 1.17 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 1.22 seconds.\n",
      "8.74 s  2.37 s per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    7.9s finished\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Use logistic regression from SGDClassifier to fit the dataset\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier(loss='log',\n",
    "                    penalty='l2',\n",
    "                    alpha=0.0001,\n",
    "                    l1_ratio=0,\n",
    "                    fit_intercept=True,\n",
    "                    verbose=1,\n",
    "                    n_jobs=1,\n",
    "                    random_state=42)\n",
    "sgd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "\n",
      "Total training time: 0.33 seconds.Total training time: 0.35 seconds.\n",
      "\n",
      "-- Epoch 2-- Epoch 2\n",
      "\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 2Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "\n",
      "\n",
      "Total training time: 0.42 seconds.Total training time: 0.39 seconds.\n",
      "\n",
      "-- Epoch 2-- Epoch 2\n",
      "\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.66 seconds.Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "\n",
      "-- Epoch 3Total training time: 0.68 seconds.\n",
      "\n",
      "-- Epoch 3\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "\n",
      "Total training time: 1.03 seconds.Total training time: 1.04 seconds.\n",
      "\n",
      "-- Epoch 4-- Epoch 4\n",
      "\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 1.36 seconds.\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831-- Epoch 5\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 1.34 seconds.\n",
      "Total training time: 1.36 seconds.\n",
      "\n",
      "-- Epoch 5-- Epoch 5\n",
      "\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 1.67 seconds.\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039Total training time: 1.65 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 1\n",
      "\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 1.72 seconds.\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 1.78 seconds.\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.80 seconds.\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 1.80 seconds.\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.77 seconds.\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "\n",
      "Total training time: 0.25 seconds.Total training time: 0.25 seconds.\n",
      "\n",
      "-- Epoch 2-- Epoch 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.8s remaining:    1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856Total training time: 0.40 seconds.\n",
      "\n",
      "Total training time: 0.40 seconds.-- Epoch 3\n",
      "\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "\n",
      "Total training time: 0.55 seconds.Total training time: 0.55 seconds.\n",
      "\n",
      "-- Epoch 4-- Epoch 4\n",
      "\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "\n",
      "Total training time: 0.70 seconds.Total training time: 0.70 seconds.\n",
      "\n",
      "-- Epoch 5-- Epoch 5\n",
      "\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259Total training time: 0.84 seconds.\n",
      "\n",
      "Total training time: 0.84 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "\n",
      "Total training time: 0.40 seconds.Total training time: 0.38 seconds.\n",
      "\n",
      "-- Epoch 2-- Epoch 2\n",
      "\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.71 seconds.\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217-- Epoch 3\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.75 seconds.\n",
      "Total training time: 0.74 seconds.\n",
      "\n",
      "-- Epoch 3-- Epoch 3\n",
      "\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "\n",
      "Total training time: 1.01 seconds.Total training time: 1.06 seconds.\n",
      "\n",
      "-- Epoch 4-- Epoch 4\n",
      "\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "\n",
      "Total training time: 1.66 seconds.Total training time: 1.72 seconds.\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.76 seconds.\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 1.72 seconds.\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.72 seconds.\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.74 seconds.\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 1.72 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.7s remaining:    1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 1.15 seconds.\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 1.12 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 1.47 seconds.\n",
      "\n",
      "Total training time: 1.51 seconds.-- Epoch 5\n",
      "\n",
      "-- Epoch 5\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 1.68 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 1.87 seconds.\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.91 seconds.\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 1.96 seconds.\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 2.00 seconds.\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 1.97 seconds.\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.99 seconds.\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.9s remaining:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 1.04 seconds.\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 1.09 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "\n",
      "Total training time: 0.37 seconds.Total training time: 0.36 seconds.\n",
      "\n",
      "-- Epoch 2-- Epoch 2\n",
      "\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 1.71 seconds.\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "\n",
      "Total training time: 1.76 seconds.Total training time: 1.78 seconds.\n",
      "\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.74 seconds.\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.73 seconds.\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 1.84 seconds.\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.7s remaining:    1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 0.78 seconds.\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 0.71 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.40 seconds.Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "\n",
      "-- Epoch 2Total training time: 0.38 seconds.\n",
      "\n",
      "-- Epoch 2\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373Total training time: 0.73 seconds.\n",
      "\n",
      "Total training time: 0.71 seconds.-- Epoch 3\n",
      "\n",
      "-- Epoch 3Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.74 seconds.\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411-- Epoch 3\n",
      "\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 1.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "\n",
      "Total training time: 1.06 seconds.Total training time: 1.13 seconds.\n",
      "\n",
      "-- Epoch 4-- Epoch 4\n",
      "\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 1.72 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 1.71 seconds.\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.73 seconds.\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.84 seconds.\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 1.83 seconds.\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "\n",
      "Total training time: 1.80 seconds.Total training time: 1.81 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.8s remaining:    1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 0.86 seconds.\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 0.80 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.27 seconds.\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332-- Epoch 2\n",
      "\n",
      "\n",
      "Total training time: 0.24 seconds.Total training time: 0.27 seconds.\n",
      "\n",
      "-- Epoch 2-- Epoch 2\n",
      "\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.57 seconds.Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "\n",
      "-- Epoch 3Total training time: 0.58 seconds.\n",
      "\n",
      "-- Epoch 3\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 0.97 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 1.15 seconds.\n",
      "\n",
      "Total training time: 1.19 seconds.-- Epoch 5\n",
      "\n",
      "-- Epoch 5\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 1.44 seconds.\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 1.50 seconds.\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 1.52 seconds.\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.55 seconds.\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 1.54 seconds.\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.54 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.5s remaining:    1.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 0.97 seconds.\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 1.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 1.09 seconds.Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "\n",
      "-- Epoch 4Total training time: 1.19 seconds.\n",
      "\n",
      "-- Epoch 4\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 1.58 seconds.\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328-- Epoch 5\n",
      "\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.84 seconds.\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 1.76 seconds.\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 1.89 seconds.\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 1.91 seconds."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.9s remaining:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 1.92 seconds.\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.96 seconds.\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.74 seconds.\n",
      "\n",
      "Total training time: 0.75 seconds.-- Epoch 5\n",
      "\n",
      "-- Epoch 5\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 1.05 seconds.\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 1.07 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1-- Epoch 1\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Norm: 5796.94, NNZs: 607, Bias: -46.334055, T: 60000, Avg. loss: 41098.270957\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7028.33, NNZs: 652, Bias: -89.924356, T: 60000, Avg. loss: 54758.953714\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7852.95, NNZs: 630, Bias: 61.991262, T: 60000, Avg. loss: 89462.399733\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6927.81, NNZs: 620, Bias: -166.589677, T: 60000, Avg. loss: 39837.371040\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4854.39, NNZs: 579, Bias: -33.612806, T: 60000, Avg. loss: 22752.272332\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7035.49, NNZs: 633, Bias: -161.196652, T: 60000, Avg. loss: 72292.659162\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6505.97, NNZs: 611, Bias: -262.417615, T: 60000, Avg. loss: 85359.730938\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6285.37, NNZs: 594, Bias: -154.973924, T: 60000, Avg. loss: 45506.399585\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4320.27, NNZs: 657, Bias: -100.981005, T: 120000, Avg. loss: 7874.997685\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4119.76, NNZs: 632, Bias: -179.142426, T: 120000, Avg. loss: 4769.617164\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4796.37, NNZs: 633, Bias: 72.848004, T: 120000, Avg. loss: 13315.601727\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3999.59, NNZs: 620, Bias: -292.711261, T: 120000, Avg. loss: 13970.962373\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3335.55, NNZs: 614, Bias: -48.292037, T: 120000, Avg. loss: 6748.068411\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3327.79, NNZs: 663, Bias: -105.947406, T: 180000, Avg. loss: 4504.099232\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3049.23, NNZs: 593, Bias: -37.362417, T: 120000, Avg. loss: 2920.786217\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3197.14, NNZs: 637, Bias: -185.925443, T: 180000, Avg. loss: 2634.354905\n",
      "Total training time: 1.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4078.91, NNZs: 648, Bias: -172.963011, T: 120000, Avg. loss: 10884.497010\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3820.65, NNZs: 606, Bias: -170.726974, T: 120000, Avg. loss: 6754.398847\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3666.28, NNZs: 644, Bias: 77.270171, T: 180000, Avg. loss: 7748.672058\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2870.27, NNZs: 621, Bias: -309.254073, T: 180000, Avg. loss: 8273.723192\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2686.57, NNZs: 664, Bias: -109.270530, T: 240000, Avg. loss: 3225.877328\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2574.64, NNZs: 626, Bias: -49.961979, T: 180000, Avg. loss: 3816.858512\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2624.19, NNZs: 639, Bias: -191.121030, T: 240000, Avg. loss: 1898.494662\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2300.55, NNZs: 613, Bias: -39.188405, T: 180000, Avg. loss: 1639.823167\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2969.74, NNZs: 644, Bias: 80.472124, T: 240000, Avg. loss: 5472.222026\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2798.27, NNZs: 614, Bias: -179.013197, T: 180000, Avg. loss: 4100.449530\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2999.91, NNZs: 655, Bias: -180.091781, T: 180000, Avg. loss: 6258.383648\n",
      "Total training time: 1.80 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2342.60, NNZs: 666, Bias: -111.639920, T: 300000, Avg. loss: 2382.077964\n",
      "Total training time: 1.81 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2359.06, NNZs: 622, Bias: -320.788179, T: 240000, Avg. loss: 5880.069878\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2309.63, NNZs: 643, Bias: -194.392899, T: 300000, Avg. loss: 1451.100039\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2510.90, NNZs: 648, Bias: 82.891730, T: 300000, Avg. loss: 4281.053557\n",
      "Total training time: 1.98 seconds.\n",
      "Norm: 2125.12, NNZs: 627, Bias: -50.531853, T: 240000, Avg. loss: 2719.130453\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1920.18, NNZs: 619, Bias: -40.583885, T: 240000, Avg. loss: 1162.415723\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2243.21, NNZs: 619, Bias: -185.240807, T: 240000, Avg. loss: 2785.884598\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2490.37, NNZs: 664, Bias: -185.098178, T: 240000, Avg. loss: 4442.622831\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7534.23, NNZs: 633, Bias: -680.711532, T: 60000, Avg. loss: 167341.581345\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2014.13, NNZs: 624, Bias: -329.656576, T: 300000, Avg. loss: 4445.575614\n",
      "Total training time: 2.18 seconds.\n",
      "Norm: 7041.42, NNZs: 633, Bias: -286.020058, T: 60000, Avg. loss: 108380.107610\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1828.20, NNZs: 631, Bias: -51.053040, T: 300000, Avg. loss: 2065.729526\n",
      "Total training time: 2.31 seconds.\n",
      "Norm: 1616.64, NNZs: 621, Bias: -41.787818, T: 300000, Avg. loss: 913.652775\n",
      "Total training time: 2.29 seconds.\n",
      "Norm: 1895.23, NNZs: 619, Bias: -189.319766, T: 300000, Avg. loss: 2114.430288\n",
      "Total training time: 2.35 seconds.\n",
      "Norm: 2036.57, NNZs: 668, Bias: -188.786746, T: 300000, Avg. loss: 3421.332960\n",
      "Total training time: 2.42 seconds.\n",
      "Norm: 4438.16, NNZs: 645, Bias: -761.714241, T: 120000, Avg. loss: 26912.034856\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    2.3s remaining:    1.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4337.35, NNZs: 645, Bias: -324.560391, T: 120000, Avg. loss: 17859.240742\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3281.61, NNZs: 653, Bias: -809.145720, T: 180000, Avg. loss: 15860.195463\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3192.84, NNZs: 656, Bias: -345.834889, T: 180000, Avg. loss: 10578.312006\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2630.73, NNZs: 655, Bias: -841.435733, T: 240000, Avg. loss: 11149.085856\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2561.56, NNZs: 657, Bias: -360.810415, T: 240000, Avg. loss: 7284.062090\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2223.75, NNZs: 659, Bias: -867.198365, T: 300000, Avg. loss: 8708.155589\n",
      "Total training time: 1.05 seconds.\n",
      "Norm: 2157.04, NNZs: 658, Bias: -372.450890, T: 300000, Avg. loss: 5700.366259\n",
      "Total training time: 0.99 seconds.\n",
      "3.65 s  332 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.8s finished\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='log',\n",
    "                    penalty='l2',\n",
    "                    alpha=0.0001,\n",
    "                    l1_ratio=0,\n",
    "                    fit_intercept=True,\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42)\n",
    "sgd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0, learning_rate='optimal',\n",
       "       loss='log', max_iter=None, n_iter=None, n_jobs=-1, penalty='l2',\n",
       "       power_t=0.5, random_state=42, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='log',\n",
    "                    penalty='l2',\n",
    "                    alpha=0.0001,\n",
    "                    l1_ratio=0,\n",
    "                    fit_intercept=True,\n",
    "                    verbose=0,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42)\n",
    "sgd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.91036269e-15   1.00296577e-15   6.60245007e-17   6.88109003e-17\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.02350044e-16   1.28583645e-15  -4.30456538e-16  -1.13089688e-16\n",
      "   8.96144501e-16  -1.06882524e-15  -4.19502396e-17  -6.97555439e-18\n",
      "  -7.04506592e-16   3.34804776e-15   1.38969553e-15  -4.45495758e-15\n",
      "  -4.01924154e-15  -2.64913936e-15   1.86924666e-16  -2.81208274e-16\n",
      "   1.76230211e-16   3.67069245e-15   8.63261083e-16  -1.32324707e-16\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00  -4.03250668e-16\n",
      "  -3.86366864e-16   4.52789368e-16  -1.92599189e-15  -7.29350504e-15\n",
      "   6.00922090e-16   7.96114794e-16   8.66803388e-16  -3.04922754e-17\n",
      "  -1.12317239e-15   3.04487916e-17  -3.53754433e-15   1.13485887e-15\n",
      "   4.12507066e-15   2.66546044e-18  -7.04250130e-15   4.09719619e-15\n",
      "   1.38073858e-15   3.04609463e-15   1.02792080e-16   1.35107088e-16\n",
      "   2.71294098e-16   1.16421456e-16   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   3.83863176e-15   1.41617502e-15\n",
      "  -6.61324092e-15   1.46757479e-15  -1.62784439e-15  -1.45036760e-17\n",
      "   2.89606996e-15  -9.80609575e-16  -1.18620576e-15  -2.35774363e-17\n",
      "   5.02965447e-15  -4.49686584e-17   5.97086084e-15   7.16543491e-16\n",
      "   3.68588493e-16   9.21938452e-17  -2.22424856e-16  -2.43513543e-16\n",
      "   1.34725564e-16   2.07157439e-16   6.38384253e-16  -9.33584228e-17\n",
      "   2.28269834e-16  -2.80412805e-15   3.26733938e-15   0.00000000e+00\n",
      "   0.00000000e+00   3.16687220e-15  -6.39714844e-17  -1.54849397e-16\n",
      "  -1.57091354e-16   1.15840231e-15   2.68956154e-17   3.96701977e-15\n",
      "  -3.08818434e-15   1.46073616e-15   1.31232525e-16   4.44882140e-15\n",
      "  -3.74351476e-16  -6.99588165e-15  -8.18233536e-15   1.83212704e-16\n",
      "   5.46436970e-16   1.06618973e-15   2.59187301e-15   3.86320050e-15\n",
      "   3.41019250e-15  -4.57849499e-16  -4.87418439e-16  -1.71809881e-15\n",
      "   3.05045757e-15   2.65301670e-17  -1.52533368e-15   4.73820427e-15\n",
      "   0.00000000e+00  -3.21661101e-18   1.02450612e-15   2.30641652e-15\n",
      "  -2.26460956e-15   3.19398813e-15   9.99436043e-15   1.40287781e-15\n",
      "   2.11279142e-15   6.16070158e-16   6.38509245e-15   4.23770648e-15\n",
      "   3.92523051e-16   3.74113703e-15   1.64824080e-15  -1.82785731e-16\n",
      "  -5.56247641e-16  -1.58085397e-15  -4.34432120e-16  -3.14759330e-16\n",
      "  -7.48160793e-17   1.71043457e-15  -3.51469964e-15   2.51355418e-16\n",
      "   1.75726100e-16  -7.63703915e-17   4.30454746e-15  -1.38678710e-16\n",
      "   0.00000000e+00  -2.64510705e-15   6.31609811e-16  -9.55200918e-15\n",
      "   9.03479143e-16   3.57982162e-17  -1.10365050e-15  -2.57371902e-15\n",
      "   2.62138459e-16  -1.11682515e-15   1.31594735e-15  -1.02155321e-16\n",
      "   9.45410417e-16   5.49893464e-16   9.20052923e-16   1.15658594e-15\n",
      "  -5.53882865e-16  -2.95191464e-15  -2.46651958e-15   4.82321590e-16\n",
      "  -1.40469858e-15   5.73245155e-16   8.54797714e-17  -5.63904016e-15\n",
      "  -3.92301099e-15   1.33616312e-15   5.80249737e-16   4.56562855e-16\n",
      "  -2.02739677e-16  -1.57203723e-15   5.03062141e-15  -4.29599643e-16\n",
      "  -1.17390357e-16  -4.36723805e-16   1.34741107e-15   1.31006317e-15\n",
      "  -2.06756094e-15   9.48826203e-16   5.42108950e-16  -1.85374864e-15\n",
      "  -4.13850435e-16   1.53189406e-15  -6.34953201e-16   4.47834362e-16\n",
      "  -8.06110734e-16  -2.97976458e-16   1.75231126e-16  -4.20645000e-16\n",
      "  -1.28038321e-16   7.43318370e-16   6.30109761e-15  -1.40737422e-16\n",
      "   3.01050851e-16  -8.67384451e-15   1.94078018e-15   1.88719781e-15\n",
      "   5.18452064e-16   2.71745358e-16   3.59434704e-18  -7.19068323e-17\n",
      "   7.72372906e-17  -1.34410076e-16   1.81469654e-16  -1.02469144e-15\n",
      "  -2.65159746e-15  -1.55715811e-15  -1.84123642e-15  -1.59006142e-16\n",
      "  -1.46730776e-15  -5.91634149e-16  -2.84002451e-16   5.37451565e-16\n",
      "  -6.04779190e-16  -3.57356737e-15  -2.02713772e-16  -1.00513486e-15\n",
      "  -3.06158802e-16  -2.06538305e-15  -5.32361192e-17   1.70240211e-15\n",
      "  -4.09350424e-15   3.34792610e-15   1.64520064e-15   4.05300423e-15\n",
      "   5.80507141e-16   1.69314424e-15   4.55781477e-16  -8.49423772e-16\n",
      "  -1.83407086e-15  -6.51072714e-16  -5.88159151e-17  -2.47229274e-15\n",
      "   1.93025410e-15   1.02000353e-15   4.98434627e-17  -5.47645262e-16\n",
      "  -1.69784742e-15  -1.64339283e-15   6.99159249e-16  -1.67747483e-15\n",
      "   2.73532123e-16   2.03063492e-16   2.18991492e-16   5.70871128e-16\n",
      "   7.77896266e-18  -1.02043744e-15  -2.67110223e-15  -8.67715159e-16\n",
      "  -8.87965627e-17  -1.15955995e-15  -2.10411688e-15  -9.62516560e-15\n",
      "  -1.42061152e-15  -4.49200862e-17   9.70103627e-18  -4.94253249e-16\n",
      "   1.45837046e-16  -2.51214790e-16  -3.57544920e-15   1.92332816e-15\n",
      "  -1.58611272e-15   3.73873155e-15   6.50868248e-16  -1.36396820e-15\n",
      "  -1.09979433e-15  -5.43431966e-16  -5.75480404e-16   9.70867831e-16\n",
      "  -5.04772150e-16  -1.48433488e-15   9.02278252e-17  -1.13498840e-15\n",
      "   1.77524107e-15   6.49848693e-16   2.45093945e-15  -2.95621860e-16\n",
      "  -2.61032816e-15   6.44306830e-16   5.19038978e-16  -2.79577923e-16\n",
      "   2.24082740e-15   9.14041065e-16  -3.52505548e-15   8.01548642e-17\n",
      "   5.20573399e-16  -7.69375304e-17   2.13675559e-15   1.31828067e-15\n",
      "   1.36187728e-15   4.29004980e-16  -1.44332694e-16  -1.43620116e-15\n",
      "   5.01269766e-15  -8.61292519e-17   8.58761210e-16   1.83331128e-16\n",
      "   2.12775538e-15   1.89825933e-16   2.57138755e-16   1.51426834e-15\n",
      "  -3.25036294e-17   1.67626098e-16   9.96334496e-16   7.19729831e-16\n",
      "  -9.54611390e-17   5.46993006e-18   1.52377312e-15   2.26241450e-15\n",
      "   5.46241388e-15  -3.20256241e-15   1.13021888e-14   1.05254833e-15\n",
      "   4.16259619e-17  -7.14437768e-17  -1.02999387e-14   6.63029631e-15\n",
      "   1.49880108e-17   2.39612034e-16   1.32078792e-15   5.25214131e-15\n",
      "  -7.02549130e-17  -4.73782125e-16   3.06393244e-15   9.26791976e-16\n",
      "   1.26852973e-15  -4.46524299e-16   7.41325519e-16  -7.20427884e-16\n",
      "   7.07952215e-16  -2.17080890e-15  -7.20627262e-18  -8.05009763e-16\n",
      "  -1.16929614e-17  -3.88998093e-16   7.42495533e-16   2.69779372e-15\n",
      "   1.56014091e-17   5.03914618e-17   1.54909534e-16   2.70488030e-16\n",
      "   3.66630106e-15  -6.84368395e-15  -3.08872187e-15  -1.17529875e-15\n",
      "  -1.20374451e-15  -1.38398922e-15   2.94984407e-16  -8.23350647e-16\n",
      "  -5.57241290e-16  -1.81873035e-17   1.68840682e-15   3.56803476e-16\n",
      "  -4.82105097e-16  -2.84070915e-16   2.45221621e-15  -6.66237436e-16\n",
      "   3.02234164e-16  -1.03560586e-15   9.53535399e-16  -2.38945623e-15\n",
      "   2.23074198e-15   1.27183565e-15  -4.08261943e-15  -2.20009141e-15\n",
      "   2.12355133e-16  -3.46657610e-15  -1.55698336e-15  -8.82551301e-15\n",
      "   2.95133270e-15   5.73580072e-16   3.40092584e-15   3.70861860e-15\n",
      "  -8.93540797e-16   1.78766261e-15  -7.12552239e-16  -2.09889143e-15\n",
      "   4.47725190e-17   1.19344904e-15   3.35254047e-16   5.07305309e-16\n",
      "  -6.55697718e-17  -6.99399797e-16   6.16144173e-16  -1.47031184e-15\n",
      "  -2.37437847e-16  -1.58291436e-15   2.67419697e-15   2.03206988e-15\n",
      "   3.34657764e-15  -2.20580591e-15  -1.44769578e-15  -8.10935809e-17\n",
      "  -8.72653960e-15  -9.44974478e-15  -3.16654573e-16  -1.41951802e-14\n",
      "  -2.97215030e-16  -9.19875287e-17   1.80749860e-16  -8.95949981e-17\n",
      "   2.00894856e-17  -6.42961610e-16  -2.21464513e-16  -1.11066711e-16\n",
      "  -4.63666143e-17   8.36960131e-17   5.11146681e-16   1.28793504e-16\n",
      "   1.57761952e-15   9.97202321e-16   2.30049313e-16  -1.47088823e-16\n",
      "   2.53489822e-16   2.85540110e-16   2.32890651e-15   8.10832882e-17\n",
      "   2.11346218e-15  -5.66043508e-16   2.17780655e-17  -2.02984736e-16\n",
      "   2.67133072e-15   2.45300542e-15   3.48882960e-17  -1.30687567e-15\n",
      "   1.93277709e-15  -8.71876645e-17  -2.02205937e-15   9.46774141e-16\n",
      "   1.54460518e-15   5.13745940e-16   5.16109377e-16  -5.81790172e-16\n",
      "   1.76255307e-16  -1.36155531e-15   3.76513635e-16   5.93277279e-16\n",
      "  -5.11120775e-16   5.90172355e-16  -1.29467548e-15  -7.40993725e-16\n",
      "  -5.69760905e-16  -1.82836801e-15  -1.22695557e-15   8.23447791e-16\n",
      "   4.82408650e-15   3.26765004e-16   1.18505946e-15  -4.15285977e-16\n",
      "   0.00000000e+00  -2.04539510e-17  -1.11037094e-15  -2.37228200e-15\n",
      "  -1.00526069e-16  -3.26705144e-15   3.15321380e-15   2.53778480e-17\n",
      "  -2.34843626e-16  -8.00800167e-16   3.29521595e-16  -1.44399307e-15\n",
      "   3.17475675e-16  -2.94686497e-16   3.76816633e-16   3.66577139e-16\n",
      "  -1.06239462e-15   7.79346957e-16  -3.75107353e-17  -6.95232760e-16\n",
      "  -3.41956093e-16   7.64536582e-16  -1.00740527e-15  -4.64739358e-16\n",
      "   4.11642017e-16  -4.31283759e-15   7.97528062e-15   1.12982002e-15\n",
      "  -9.31555180e-18   2.82366066e-15  -3.50214788e-15  -8.20582491e-16\n",
      "   6.49779304e-16   2.56556720e-15   2.99351285e-16  -7.30911628e-16\n",
      "   1.75401823e-15  -3.87956334e-16   2.90608278e-16   2.24833300e-15\n",
      "  -4.59336273e-17   4.68084830e-16  -2.54529731e-16   1.30336806e-15\n",
      "   1.21679333e-15  -2.09774790e-15   9.04768852e-16  -4.95540646e-16\n",
      "  -2.33346675e-16  -2.14643118e-19  -6.02629058e-17  -4.32545851e-15\n",
      "  -1.58242308e-15   5.91526828e-16   2.99833885e-16  -6.22576643e-17\n",
      "   3.40148696e-15  -2.17032786e-15  -6.78501630e-15  -2.98958543e-16\n",
      "  -3.48288990e-16  -3.53554130e-15  -1.14358708e-15  -4.27574642e-16\n",
      "   5.81957630e-16  -4.42087108e-16   1.93104791e-17  -9.13343475e-17\n",
      "  -3.37359770e-16   1.41388753e-16   9.64067715e-16   4.09624186e-16\n",
      "  -4.68887891e-16   2.32097489e-15  -5.70025508e-17  -2.69217981e-16\n",
      "   3.35435383e-17  -6.51848945e-17  -3.09186750e-15   1.04583009e-16\n",
      "  -1.50945922e-15  -5.98632255e-17   3.41491314e-15  -4.20408355e-16\n",
      "   0.00000000e+00   5.50614346e-15  -7.81848660e-16  -6.99953984e-17\n",
      "   8.24472897e-16   1.30651601e-15   1.28276649e-15   1.14873666e-15\n",
      "  -3.21872159e-17   1.25583988e-15  -1.03789570e-15   1.05299103e-16\n",
      "   9.24627504e-16  -7.95273569e-17   1.68246898e-16  -1.76799316e-16\n",
      "  -6.15407725e-16   8.99069708e-16  -5.79536419e-17   1.52629761e-16\n",
      "  -2.88186789e-15   8.97210084e-16  -2.56854491e-16   2.00273872e-15\n",
      "   2.49430106e-16  -1.79700699e-15  -8.61358034e-15  -3.55965708e-15\n",
      "  -1.12748858e-15   2.34791064e-16  -1.16165341e-15  -1.22563302e-15\n",
      "   3.42796254e-15  -1.30202886e-15   5.66443189e-16   1.03398771e-16\n",
      "   9.04591217e-17   1.27725006e-15  -9.12777261e-16   3.23518989e-17\n",
      "   7.77000686e-16  -6.68539298e-17  -4.34300744e-16   1.75764588e-15\n",
      "   8.74448662e-17  -1.60329342e-15  -2.28709644e-16   1.01153345e-15\n",
      "   2.40216920e-15  -6.96080230e-16  -6.54395982e-16  -2.54674060e-16\n",
      "   3.15304958e-15   2.96052881e-15   2.96059323e-15  -2.63484538e-15\n",
      "  -3.28163683e-16   4.75837541e-18  -5.03023549e-17   1.93710788e-17\n",
      "   4.93406889e-15   2.06673567e-16  -8.28866605e-16  -1.02246822e-15\n",
      "  -3.24577402e-16   5.05312459e-16  -9.71217551e-16   9.68118178e-16\n",
      "   1.09842135e-15  -4.32729778e-16   2.14117520e-15  -8.71073584e-16\n",
      "  -4.37484493e-15  -1.73111525e-16  -7.09680463e-16  -2.92022887e-15\n",
      "  -4.34806265e-15  -7.55135769e-16  -3.07374496e-17  -1.99729122e-17\n",
      "  -2.08450387e-16  -4.10877235e-15  -3.22543051e-15  -2.64329392e-15\n",
      "   0.00000000e+00  -3.21661101e-18  -1.30386778e-15  -4.39405456e-17\n",
      "   2.76939906e-15  -4.76207962e-16  -1.02425476e-16  -1.09719641e-16\n",
      "   9.31967466e-16   5.19143987e-16   1.17608516e-15   2.26922925e-15\n",
      "  -7.49048971e-16   1.61574458e-17  -6.56389757e-16  -1.33804079e-16\n",
      "   3.05962663e-16   7.86668878e-16   4.76515494e-15  -7.38751467e-15\n",
      "   2.24153103e-16   5.09601620e-17   8.68421076e-17   2.64856193e-16\n",
      "  -1.09070160e-17  -1.33818882e-16   4.28123925e-15   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -3.37624604e-17   8.04192014e-16\n",
      "  -3.76521707e-15  -3.23948229e-15  -2.28368250e-16   1.32520384e-16\n",
      "   6.26521057e-16  -5.66969619e-16  -6.56387907e-16   3.95615392e-15\n",
      "   3.66607485e-15  -2.66607107e-16   4.85652259e-16  -1.73341896e-15\n",
      "   2.57679988e-15  -1.28603054e-15  -8.18530429e-17   3.47527562e-17\n",
      "   1.24306584e-15   1.15662110e-17   8.36238486e-17   1.04648790e-15\n",
      "  -9.80617034e-15  -3.32478697e-15   2.28931324e-15   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.47207794e-16   4.20605853e-16\n",
      "  -1.60048144e-15   1.16639268e-15   2.07948520e-15  -6.59187519e-16\n",
      "  -3.28044351e-15  -3.05311332e-18  -8.04710002e-16  -1.83716005e-16\n",
      "  -1.14705282e-15   4.67574128e-16  -6.27800589e-15  -2.18632519e-16\n",
      "  -1.26800052e-15  -4.66333453e-16  -8.57027412e-17  -2.70689952e-15\n",
      "   5.12659359e-17   2.22100116e-17   5.55284453e-15  -9.32087648e-15\n",
      "   3.59615925e-16  -2.33157767e-15  -3.92188018e-16   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   5.79703531e-17  -5.79646794e-15  -2.79798939e-15  -3.49512086e-17\n",
      "  -1.24250147e-16   2.43531954e-15   1.11895678e-16  -1.47824068e-15\n",
      "  -5.38942964e-16   6.36019940e-16  -6.74646450e-16   1.17618878e-15\n",
      "  -3.04859841e-16  -1.40362259e-15   4.97583456e-17   1.10314397e-15\n",
      "  -1.07032161e-15   3.09001459e-15   3.16015154e-16   6.74032589e-18\n",
      "  -1.59765763e-15  -1.23233859e-16   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.25720325e-17   1.19827117e-15   2.37543307e-15  -1.92683092e-15\n",
      "  -3.53711620e-16   5.95937304e-16  -1.28601831e-14   2.96651592e-16\n",
      "  -1.83769527e-15   2.95324644e-16  -2.96511658e-16  -6.66443752e-17\n",
      "  -2.32621792e-17  -2.40226681e-15  -5.50191837e-17   6.07102378e-15\n",
      "  -1.38039904e-15   1.53672503e-16   8.89778037e-16  -2.10333906e-15\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "## scale the input features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "sclr = StandardScaler()\n",
    "X_train = sclr.fit_transform(X_train)\n",
    "print(np.mean(X_train, axis=0))\n",
    "print(np.mean(X_train, axis=0).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0, learning_rate='optimal',\n",
       "       loss='log', max_iter=None, n_iter=None, n_jobs=-1, penalty='l2',\n",
       "       power_t=0.5, random_state=42, shuffle=True, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the dataset\n",
    "sgd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\Amanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\Amanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90718608  0.91025     0.91211318]\n",
      "accuracy: 0.909849753645\n"
     ]
    }
   ],
   "source": [
    "# perform cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(sgd,X_train,y_train,cv=3,scoring = 'accuracy')\n",
    "scores.sort()\n",
    "accuracy = scores.mean()\n",
    "print(scores)\n",
    "print('accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\Amanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\Amanda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5749    3   23   10   10   36   50    8   29    3]\n",
      " [   1 6473   41   26    6   40   10   18  128   10]\n",
      " [  51   33 5319  113   94   24   91   62  189   15]\n",
      " [  46   31  133 5371    3  227   36   51  136   99]\n",
      " [  16   26   42    8 5403    8   59   35   84  215]\n",
      " [  69   36   37  182   83 4591  105   30  171   90]\n",
      " [  31   27   53    3   42   99 5535    7   46    1]\n",
      " [  32   21   75   26   64    9    2 5763   22  210]\n",
      " [  53  133   68  138   21  157   63   33 5040  133]\n",
      " [  47   31   30   80  155   36    2  199   78 5347]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd, X_train,y_train,cv=3)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17800f20358>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAACwZJREFUeJzt3c+LXfUZx/HPJ/PDMbFFQ7sxMzQK\n1dYflIShqIEsjIu2itkUtKBQN9m0GkUQ7cZ/QEQXRRhi3Rh0EbMIItaCuihC6DiJxHEsiNokGjEl\nVMXFZH48XcwNqEnnnqn3uWdunvcLhMx4/PJ4ue85596c+x1HhADUsqHtAQD0H+EDBRE+UBDhAwUR\nPlAQ4QMFtRa+7V/Z/qftD2w/2tYcTdmesP2G7Tnbs7b3tj1TE7aHbB+x/XLbszRh+3LbB2y/33ms\nb257pm5sP9R5Trxr+wXbY23P1E0r4dsekvRnSb+WdJ2k39m+ro1Z1mBR0sMR8XNJN0n6wwDMLEl7\nJc21PcQaPC3p1Yj4maRfaJ3PbnuLpAckTUbEDZKGJN3d7lTdtXXG/6WkDyLiw4g4K+lFSbtbmqWR\niDgVETOdP3+llSfklnanWp3tcUm3S9rX9ixN2P6hpJ2SnpWkiDgbEf9pd6pGhiVdantY0kZJn7Y8\nT1dthb9F0olvfH1S6zyib7K9VdI2SYfbnaSrpyQ9Imm57UEaulrSaUnPdV6e7LO9qe2hVhMRn0h6\nQtJxSackfRERr7U7VXdthe8LfG8g7h22fZmklyQ9GBFftj3P/2L7DkmfR8Tbbc+yBsOStkt6JiK2\nSfpa0rp+/8f2FVq5Wr1K0pWSNtm+p92pumsr/JOSJr7x9bgG4PLI9ohWot8fEQfbnqeLHZLutP2x\nVl5K3Wr7+XZH6uqkpJMRce5K6oBWfhCsZ7dJ+igiTkfEgqSDkm5peaau2gr/H5J+avsq26NaeTPk\nUEuzNGLbWnntORcRT7Y9TzcR8VhEjEfEVq08vq9HxLo+E0XEZ5JO2L62861dkt5rcaQmjku6yfbG\nznNkl9b5G5LSyqVV30XEou0/SvqrVt4F/UtEzLYxyxrskHSvpGO2j3a+96eIeKXFmS5G90va3zkh\nfCjpvpbnWVVEHLZ9QNKMVv7m54ikqXan6s58LBeohzv3gIIIHyiI8IGCCB8oiPCBgloP3/aetmdY\ni0GbV2Lmfhi0eVsPX9JAPWAavHklZu6HgZp3PYQPoM9SbuDZvHlzjI+PNzr2zJkz2rx5c6Njjx07\n9n3GAkqIiAt9CO5bUm7ZHR8f1yuv9P5O1omJie4H4XtbueV8sGTdgZr5WLR51yyX+kBBhA8URPhA\nQYQPFET4QEGNwh+0PfABrK5r+AO6Bz6AVTQ54w/cHvgAVtck/IHeAx/A+ZqE32gPfNt7bE/bnj5z\n5sz3nwxAmibhN9oDPyKmImIyIiab3nsPoB1Nwh+4PfABrK7rh3QGdA98AKto9Om8zi+N4BdHABcJ\n7twDCiJ8oCDCBwoifKAgwgcKStls03bKZmKZe5Rt2JDzM3AQfxtx1j5zg/hYjIyMpK29sLCQsm6T\nzTY54wMFET5QEOEDBRE+UBDhAwURPlAQ4QMFET5QEOEDBRE+UBDhAwURPlAQ4QMFET5QEOEDBRE+\nUBDhAwURPlAQ4QMFET5QEOEDBRE+UFCjX5r5/8jYojlrC2xJeuedd1LW3b59e8q6Ut521UtLSynr\nDg0Npawr5T0Wmc+5Nl2c/1cAVkX4QEGEDxRE+EBBhA8URPhAQYQPFNQ1fNsTtt+wPWd71vbefgwG\nIE+TG3gWJT0cETO2fyDpbdt/i4j3kmcDkKTrGT8iTkXETOfPX0mak7QlezAAedb0Gt/2VknbJB3O\nGAZAfzS+V9/2ZZJekvRgRHx5gX+/R9KeHs4GIEmj8G2PaCX6/RFx8ELHRMSUpKnO8TmfmADQE03e\n1bekZyXNRcST+SMByNbkNf4OSfdKutX20c4/v0meC0Cirpf6EfF3Sb3/cD2A1nDnHlAQ4QMFET5Q\nEOEDBRE+UJAzdie1HRm77GYaHs7ZcHhmZiZlXUm68cYbU9YdGxtLWXd+fj5l3UxZzwtJWl5e7vma\nS0tLioiu8XHGBwoifKAgwgcKInygIMIHCiJ8oCDCBwoifKAgwgcKInygIMIHCiJ8oCDCBwoifKAg\nwgcKInygIMIHCiJ8oCDCBwoifKAgwgcKInygoLTttXu+aLKs7cAzHt9zZmdnU9a9/vrrU9bdsCHv\nPJP1OGfOPDIy0vM15+fntby8zPbaAM5H+EBBhA8URPhAQYQPFET4QEGEDxTUOHzbQ7aP2H45cyAA\n+dZyxt8raS5rEAD90yh82+OSbpe0L3ccAP3Q9Iz/lKRHJC0nzgKgT7qGb/sOSZ9HxNtdjttje9r2\ndM+mA5CiyRl/h6Q7bX8s6UVJt9p+/rsHRcRURExGxGSPZwTQY13Dj4jHImI8IrZKulvS6xFxT/pk\nANLw9/hAQcNrOTgi3pT0ZsokAPqGMz5QEOEDBRE+UBDhAwURPlBQ2i67GbuTZu5Ym2V0dDRt7cXF\nxZR1Dx06lLLu7t27U9aVpKWlpZR1M3bCPSdj5qWlJUUEu+wCOB/hAwURPlAQ4QMFET5QEOEDBRE+\nUBDhAwURPlAQ4QMFET5QEOEDBRE+UBDhAwURPlAQ4QMFET5QEOEDBRE+UBDhAwURPlBQ2i67dteN\nPteVrHmXl5dT1pXyZs7YIVmS5ubmUtaVpGuuuSZl3czncdau0eyyC+CCCB8oiPCBgggfKIjwgYII\nHyiI8IGCGoVv+3LbB2y/b3vO9s3ZgwHIM9zwuKclvRoRv7U9Kmlj4kwAknUN3/YPJe2U9HtJioiz\nks7mjgUgU5NL/aslnZb0nO0jtvfZ3pQ8F4BETcIflrRd0jMRsU3S15Ie/e5BtvfYnrY93eMZAfRY\nk/BPSjoZEYc7Xx/Qyg+Cb4mIqYiYjIjJXg4IoPe6hh8Rn0k6Yfvazrd2SXovdSoAqZq+q3+/pP2d\nd/Q/lHRf3kgAsjUKPyKOSuISHrhIcOceUBDhAwURPlAQ4QMFET5QEOEDBQ3U9tpZ2xFLeVtKZ26v\nPYgzZzlx4kTKuhMTEynrStLY2FjP15yfn9fy8jLbawM4H+EDBRE+UBDhAwURPlAQ4QMFET5QEOED\nBRE+UBDhAwURPlAQ4QMFET5QEOEDBRE+UBDhAwURPlAQ4QMFET5QEOEDBRE+UFDaLrsZO8AODQ31\nfM1zlpaWUtbNnHlhYSFl3dHR0ZR1FxcXU9aV8nZgfuutt1LWlaSdO3f2fM3FxUV22QVwYYQPFET4\nQEGEDxRE+EBBhA8URPhAQY3Ct/2Q7Vnb79p+wXbvf80ngL7pGr7tLZIekDQZETdIGpJ0d/ZgAPI0\nvdQflnSp7WFJGyV9mjcSgGxdw4+ITyQ9Iem4pFOSvoiI17IHA5CnyaX+FZJ2S7pK0pWSNtm+5wLH\n7bE9bXu692MC6KUml/q3SfooIk5HxIKkg5Ju+e5BETEVEZMRMdnrIQH0VpPwj0u6yfZG25a0S9Jc\n7lgAMjV5jX9Y0gFJM5KOdf6bqeS5ACQabnJQRDwu6fHkWQD0CXfuAQURPlAQ4QMFET5QEOEDBRE+\nUFDa9tor9/ogY5vxc7K27s7atjtrC2xJuuSSS1LWzdwS/OjRoz1f86677tLs7CzbawM4H+EDBRE+\nUBDhAwURPlAQ4QMFET5QEOEDBRE+UBDhAwURPlAQ4QMFET5QEOEDBRE+UBDhAwURPlAQ4QMFET5Q\nEOEDBRE+UFDWLrunJf2r4eE/kvTvng+RZ9DmlZi5H9bLvD+JiB93Oygl/LWwPR0Rk60OsQaDNq/E\nzP0waPNyqQ8URPhAQesh/Km2B1ijQZtXYuZ+GKh5W3+ND6D/1sMZH0CfET5QEOEDBRE+UBDhAwX9\nF67Qs1zOxbc9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17800debc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot confusion matrix using plt.matshow()\n",
    "# use gray scale by setting cmap=plt.cm.gray\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "plt.matshow(confusion_matrix(y_train,y_train_pred),cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17803585b70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4y/fQRgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17800deb898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Which mis-classification happens most frequently?\n",
    "# fill diagonal on confusion matrix with 0 (np.fill_diagonal)\n",
    "conf_matrix1 = confusion_matrix(y_train,y_train_pred)\n",
    "np.fill_diagonal(conf_matrix1,0)\n",
    "plt.matshow(conf_matrix1, cmap=plt.cm.gray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
