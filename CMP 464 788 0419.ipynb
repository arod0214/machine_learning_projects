{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "## Linear Regression with Batch Gradient Descent\n",
    "\n",
    "Another way to train the model is the Batch Gradient Descent method. Recall the following facts:\n",
    "- **Gradient**: the vector composed of all partial derivatives. For example, let $L(x, y, z) = x^2 + xy + yz$, its partial derivatives are\n",
    "\n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial L}{\\partial x} =& 2x + y\\\\\n",
    "\\frac{\\partial L}{\\partial y} =& x + z\\\\\n",
    "\\frac{\\partial L}{\\partial z} =& y\\\\\n",
    "\\end{array}\n",
    "\n",
    "Therefore if $(x, y, z) = (1, 2, 3)$, its gradient vector is $(2x + y, x + z, y)=(4, 4, 2)$.\n",
    "\n",
    "- **Gradient Descent**: a method to find a *local minimum* of a function. It is based on the observation that the value of the function decreases fastest if the point goes in the direction of the negative gradient.\n",
    "\n",
    "![](Data/BGD_1.png)\n",
    "- **Gradient Descent algorithm**: \n",
    "\n",
    "repeat until converge{\n",
    "\n",
    "    parameter <-- parameter - (learning_rate) * (partial derivative with respect to this parameter)\n",
    "    \n",
    "}\n",
    "\n",
    "- **Batch vs. Stochastic**: Since the cost function for a machine learning task is usually the average of the costs casued by each training instance, its gradient vector will be the average of the gradients of each individual cost. Batch Gradient Descent means that one uses the gradient of the cost function to perform gradient descent. Stochastic Gradient Descent means that one uses the gradient of one randomly chosen instance to perform gradient descnet, hoping that this gradient is *close* to the average gradient. SGD is much faster than BGD, but its performance is less stable.\n",
    "\n",
    "- **Mini-Batch Stochastic Gradient Descent**: a variation of SGD that splits the training set into small batches that in each iteration, one batch is used to calculate the gradient.\n",
    "\n",
    "In this example we will first conduct gradient descent using manually computed gradients, then we will use TensorFlow's autodiff reature to let TensorFlow compute the gradients automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to C:\\Users\\Amanda\\scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "# Load California housing data\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Manually computing the gradients\n",
    "- Cost generated from each instance $(\\textbf{x}^{(i)}, y^{(i)})$ is: \n",
    "\n",
    "$(\\theta\\cdot\\textbf{x}^{(i)} - y^{(i)})^2 = (\\theta_0\\cdot 1 + \\theta_1x^{(i)}_1 + \\theta_2x^{(i)}_2 + \\cdots + \\theta_nx^{(i)}_n - y^{(i)})^2$.\n",
    "- Its partial derivative with respect to $\\theta_j$ is:\n",
    "\n",
    "$2x^{(i)}_j(\\theta_0\\cdot 1 + \\theta_1x^{(i)}_1 + \\theta_2x^{(i)}_2 + \\cdots + \\theta_nx^{(i)}_n - y^{(i)})$.\n",
    "- Average cost is: $\\frac{1}{m}\\sum_{i=1}^m(y^{(i)} - \\theta\\cdot\\textbf{x}^{(i)})^2$.\n",
    "- The partial derivative of the average cost with respect to $w_i$ is:\n",
    "\n",
    "$\\frac{1}{m}\\sum_{i=1}^m2x^{(i)}_j(\\theta_0\\cdot 1 + \\theta_1x^{(i)}_1 + \\theta_2x^{(i)}_2 + \\cdots + \\theta_nx^{(i)}_n - y^{(i)})$.\n",
    "- The update rule of gradient descent is: \n",
    "\n",
    "$\\theta_j = \\theta_j - \\textit{(learning_rate)}\\cdot\\textit{partial derivative}$.\n",
    "\n",
    "The formula is\n",
    "\n",
    "$\\theta_j = \\theta_j - \\textit{learning_rate}\\cdot\\frac{1}{m}\\sum_{i=1}^m2x^{(i)}_j(\\theta_0\\cdot 1 + \\theta_1x^{(i)}_1 + \\theta_2x^{(i)}_2 + \\cdots + \\theta_nx^{(i)}_n - y^{(i)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4cf42a963e03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Reset dataflow graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Reset dataflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000  # each training instance will be used 1000 times during training phase\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training data\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "# Define theta variables with random initialization\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "\n",
    "# Use y_pred to store the prediction theta*x\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "\n",
    "# Use tf.square() and tf.reduce_mean() to construct the average cost\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# Calculate gradients using formula\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "\n",
    "# Use tf.assign() to define the update rule\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "# initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    print('best_theta:')\n",
    "    print(best_theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using autodiff\n",
    "\n",
    "Use tf.gradients(ys, xs) to ask TensorFlow automatically compute the derivatives of sum of ys with respect to xs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using a TensorFlow Optimizer\n",
    "TensorFlow also provides a number of optimizers out of the box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use tf.train.GradientDescentOptimizer() and \n",
    "# its minimize() method to perform Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TensorBoard to Visualize Dataflow Graph\n",
    "So far we are relying on the print() function to visualize progress during training. There is a better way: use TensorBoard. If you feed it some training stats, TensorBoard will display nice interactive visualizations of these stats in a web browser. This is very useful to identify errors in the graph, to find bottlenecks, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome:  222\n",
      "outcome:  222\n",
      "outcome:  222\n",
      "outcome:  222\n",
      "outcome:  222\n",
      "outcome:  222\n",
      "outcome:  222\n",
      "outcome:  222\n",
      "outcome:  222\n",
      "outcome:  222\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize a simple graph on TensorBoard\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant(10, name='a')\n",
    "b = tf.constant(20, name='b')\n",
    "c = a * b + b + 2\n",
    "\n",
    "# location of log directory\n",
    "logdir = './test_log'\n",
    "\n",
    "# add a summary node\n",
    "c_summary = tf.summary.scalar('c_summary', c)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create a writer object\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "#     result = sess.run(c)\n",
    "#     summary_value = c_summary.eval()\n",
    "#     writer.add_summary(summary_value)\n",
    "#     print('outcome: ', result)\n",
    "\n",
    "    for k in range(10):\n",
    "        result = sess.run(c)\n",
    "        summary_value = c_summary.eval()\n",
    "        writer.add_summary(summary_value, k)\n",
    "        print('outcome: ', result)    \n",
    " \n",
    "# Close FileWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create **summary nodes** to store values that you want to visualize.\n",
    "- Create a directory to save the log files.\n",
    "- Create a **FileWriter** and use it to save the summary values.\n",
    "- Close FileWriter at the end of the program.\n",
    "Then open command line (Anaconda prompt is recommended) and execute the following command:\n",
    "\n",
    "    **tensorboard --logdir (enter directory here)**\n",
    "- Graph tab will visualize the dataflow graph\n",
    "- Scalar tab will uisualize the summary stats that you saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use TensorBoard to visualize gradient descent.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Use date and time to name the log directory\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "# training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = scaled_housing_data_plus_bias\n",
    "y_data = housing.target.reshape(-1, 1)\n",
    "with tf.Session() as sess:                                                        \n",
    "    sess.run(init)                                                               \n",
    "\n",
    "    for epoch in range(n_epochs):                      \n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval(feed_dict={X:X_data, y:y_data}))\n",
    "            summary_str = mse_summary.eval(feed_dict={X:X_data, y:y_data})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "        sess.run(training_op, feed_dict={X:X_data, y:y_data})\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print('best theta:', best_theta)\n",
    "file_writer.close()                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring Models\n",
    "\n",
    "To save a model:\n",
    "- Create a **Saver** node after all variable nodes are created.\n",
    "- In the execution phase, call its save() method to save the model\n",
    "\n",
    "To restore a model:\n",
    "- Create a Saver node at the end of the construction phase.\n",
    "- At the beginning of the execution phase, instead of initializing the variables using the init node, call the restore() method of the Saver object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       # not shown in the book\n",
    "learning_rate = 0.01                                                                  # not shown\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            # not shown\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")            # not shown\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      # not shown\n",
    "error = y_pred - y                                                                    # not shown\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    # not shown\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            # not shown\n",
    "training_op = optimizer.minimize(mse)                                                 # not shown\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                                # not shown\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval() \n",
    "    print('bets theta:')\n",
    "    print(best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (optional)\n",
    "Perform Mini-Batch SGD on California housing data, set the size of mini batch to be 100. Visualize the learning curve on TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
